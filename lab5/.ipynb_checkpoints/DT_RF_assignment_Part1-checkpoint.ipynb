{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5\n",
    "## Decision Trees and Random Forests for Regression, Part 1\n",
    "\n",
    "### About this notebook\n",
    "\n",
    "The general description and instructions as well as questions for the walk through Part 1 of the task (this notebook) are found in the Assignment description in Canvas!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU DON'T HAVE TO RUN THIS IF EVERYTHING IS ALREADY INSTALLED CORRECTLY\n",
    "!pip3 install --upgrade pip\n",
    "!pip3 install graphviz\n",
    "!pip3 install dtreeviz\n",
    "!pip3 install numpy scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset(s)\n",
    "\n",
    "**Step 0:** First, load the dataset. Ultimately, you should be working with the California housing data, but for quicker test runs, it might help to first start out with the Diabetes data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run time 0.8s\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from ConceptDataRegr import ConceptDataRegr\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "test_case = 'diabetes'\n",
    "#test_case = 'california'\n",
    "\n",
    "if test_case == 'california':\n",
    "    dataset = fetch_california_housing()\n",
    "elif test_case == 'diabetes':\n",
    "    dataset = load_diabetes()\n",
    "else:\n",
    "    raise ValueError('Unknown test case')\n",
    "\n",
    "X = dataset.data\n",
    "y = dataset.target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** Get some information about the dataset you're looking at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_case == 'california' :\n",
    "    print(\"target:\", list(dataset.target_names))\n",
    "print(\"features:\", list(dataset.feature_names))\n",
    "print(\"description:\", dataset.DESCR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** Split the data into train, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run time 0.7s\n",
    "\n",
    "train_ratio = 0.70\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "X = dataset.data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1 - train_ratio, random_state=0)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=test_ratio/(test_ratio + validation_ratio), random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Regressor\n",
    "\n",
    "Run the cells below and inspect the output. Use the documentation where needed. Be prepared to answer \"random\" questions posed by the TA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run time 0.7s\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "regressor1 = DecisionTreeRegressor(random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** Now let's examine the decision tree. \n",
    "Check out [cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)\n",
    "and [DecisionTreeRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html?highlight=decision+tree)\n",
    "to learn about those tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run time 1.8s\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_score(regressor1, X_train, y_train, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run time 0.3s\n",
    "\n",
    "regressor1.fit(X_train, y_train)\n",
    "regressor1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4:** Let's have a look at two other parameters, max_depth and min_samples_leaf.\n",
    "How do you interpret the following numbers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run time 0.2s\n",
    "\n",
    "regressor2 = DecisionTreeRegressor(max_depth=1, random_state=0)\n",
    "cross_val_score(regressor2, X_train, y_train, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run time 0.1s\n",
    "\n",
    "regressor2.fit(X_train, y_train)\n",
    "regressor2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run time 1.2s\n",
    "\n",
    "regressor3 = DecisionTreeRegressor(min_samples_leaf=20, random_state=0)\n",
    "cross_val_score(regressor3, X_train, y_train, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run time 0.1s\n",
    "\n",
    "regressor3.fit(X_train, y_train)\n",
    "regressor3.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5:** The next cells give examples how to visualize regressor2 and regressor3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "#run time 0.2s\n",
    "\n",
    "from sklearn import tree\n",
    "import graphviz\n",
    "from IPython.display import Image\n",
    "\n",
    "dot_data = tree.export_graphviz(regressor2, feature_names=dataset.feature_names, out_file=None, filled=True, rounded=True, special_characters=True)\n",
    "graph = graphviz.Source(dot_data, format=\"png\") \n",
    "graph.render(\"decision_tree_regressor2\")\n",
    "Image(\"decision_tree_regressor2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "#run time 4.8s\n",
    "\n",
    "dot_data = tree.export_graphviz(regressor3, feature_names=dataset.feature_names, out_file=None, filled=True, rounded=True, special_characters=True)\n",
    "graph = graphviz.Source(dot_data, format=\"png\") \n",
    "graph.render(\"decision_tree_regressor3\")\n",
    "Image(\"decision_tree_regressor3.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6:** Another nice way to visualize the decision trees nicely is with dtreeviz. To make these plots it takes quite some time, so we recommend to use this visualization tool for trees with few nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run time 6.9s\n",
    "\n",
    "from dtreeviz.trees import dtreeviz\n",
    "\n",
    "viz = dtreeviz(regressor2, X, y,\n",
    "                target_name=\"target\",\n",
    "                feature_names=dataset.feature_names)\n",
    "#viz.view()\n",
    "# this opens the visualization in a new window. If you want to display the output inside the notebook use:\n",
    "viz\n",
    "# if you want to store the output in a file use:\n",
    "#viz.save(\"dtreeviz.svg\")\n",
    "# instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainability\n",
    "\n",
    "**Step 7:** If you want to visualize (explain) the decision path for one prediction, you can also use dtreeviz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run time 6.8s\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sample = X_test[np.random.randint(0, len(X_test)),:] # random sample from training\n",
    "\n",
    "viz = dtreeviz(regressor2, X, y,\n",
    "                target_name=\"target\",\n",
    "                feature_names=dataset.feature_names,\n",
    "                X=sample)\n",
    "#viz.view()\n",
    "viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 8:** For bigger graphs you just show the decision path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run time 10.4s\n",
    "\n",
    "viz = dtreeviz(regressor3, X, y,\n",
    "                target_name=\"target\",\n",
    "                feature_names=dataset.feature_names,\n",
    "                X=sample,\n",
    "                show_just_path=True)\n",
    "#viz.view()\n",
    "viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 9:** Another option to explain the prediction for big trees is this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run time 0.1s\n",
    "\n",
    "from dtreeviz.trees import explain_prediction_path\n",
    "\n",
    "print(explain_prediction_path(regressor3, sample, feature_names=dataset.feature_names, explanation_type=\"plain_english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Pruning\n",
    "\n",
    "### Cost Complexity Pruning\n",
    "\n",
    "A smart way of pruning is to use cost complexity pruning. This method is based on the idea that a tree with a lot of nodes is more likely to overfit than a tree with few nodes. Therefore, we can prune the tree by removing nodes that are not important for the prediction. The cost complexity pruning method uses a parameter $\\alpha$ to determine how many nodes to remove. It basically is a tradeoff between having a tree with many nodes that has a small total MSE, vs. a tree with fewer nodes but greater total MSE. The following code shows how to use the cost complexity pruning method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find the alphas that change the Decision Tree to be \"cut down\" and we record the worsening of the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run time 0.8s\n",
    "\n",
    "path = regressor1.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then plot the MSE for each $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "# run time 0.4s\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(ccp_alphas[:-1], impurities[:-1], marker=\"o\", drawstyle=\"steps-post\")\n",
    "ax.set_xlabel(\"effective alpha\")\n",
    "ax.set_ylabel(\"total impurity of leaves\")\n",
    "ax.set_title(\"Total Impurity vs effective alpha for training set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now train a Decision Tree for each $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run time 0.2s\n",
    "\n",
    "regressors = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    regressor = DecisionTreeRegressor(min_samples_leaf=20, random_state=0, ccp_alpha=ccp_alpha)\n",
    "    regressor.fit(X_train, y_train)\n",
    "    regressors.append(regressor)\n",
    "print(\n",
    "    \"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n",
    "        regressors[-1].tree_.node_count, ccp_alphas[-1]\n",
    "    ),\n",
    ")\n",
    "if regressors[-1].tree_.node_count == 1:\n",
    "    print(\"Removing last node.\")\n",
    "    regressors = regressors[:-1]\n",
    "    ccp_alphas = ccp_alphas[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run time 0.5s\n",
    "\n",
    "node_counts = [regressor.tree_.node_count for regressor in regressors]\n",
    "depth = [regressor.tree_.max_depth for regressor in regressors]\n",
    "fig, ax = plt.subplots(2, 1)\n",
    "ax[0].plot(ccp_alphas, node_counts, marker=\"o\", drawstyle=\"steps-post\")\n",
    "ax[0].set_xlabel(\"alpha\")\n",
    "ax[0].set_ylabel(\"number of nodes\")\n",
    "ax[0].set_title(\"Number of nodes vs alpha\")\n",
    "ax[1].plot(ccp_alphas, depth, marker=\"o\", drawstyle=\"steps-post\")\n",
    "ax[1].set_xlabel(\"alpha\")\n",
    "ax[1].set_ylabel(\"depth of tree\")\n",
    "ax[1].set_title(\"Depth vs alpha\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a way to get all the scores for each tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run time 0.6s\n",
    "\n",
    "train_scores = [regressor.score(X_train, y_train) for regressor in regressors]\n",
    "val_scores = [regressor.score(X_val, y_val) for regressor in regressors]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"alpha\")\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.set_title(\"Accuracy vs alpha for training and validation sets\")\n",
    "ax.plot(ccp_alphas, train_scores, marker=\"o\", label=\"train\", drawstyle=\"steps-post\")\n",
    "ax.plot(ccp_alphas, val_scores, marker=\"o\", label=\"val\", drawstyle=\"steps-post\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best tree is the one with the highest score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run time 0.8s\n",
    "\n",
    "idx_max = np.argmax(val_scores)\n",
    "regressor_best = regressors[idx_max]\n",
    "print(\"Best alpha: {}\".format(ccp_alphas[idx_max]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run time 0.7s\n",
    "\n",
    "regressor_best.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run time 0.8s\n",
    "\n",
    "dot_data = tree.export_graphviz(regressor_best, feature_names=dataset.feature_names, out_file=None, filled=True, rounded=True, special_characters=True)\n",
    "graph = graphviz.Source(dot_data, format=\"png\") \n",
    "graph.render(\"decision_tree_regressor_best\")\n",
    "Image(\"decision_tree_regressor_best.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Ensemble methods: \n",
    "\n",
    "Experiment with **at least two methods that are not the VotingRegressor**, which is only an example, and that are **NOT random forests**. Inspect the documentation of the different estimators. Note that you can use regressors as estimators within an ensemble that are themselves based on an ensemble. Below is an **example** for a (tiny) voting ensemble. Visualise your results to be able to discuss them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run time 1.2s\n",
    "\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "voting=VotingRegressor(estimators=[('lr', LinearRegression()), ('dt', DecisionTreeRegressor())])\n",
    "voting.fit(X_train, y_train)\n",
    "voting.score(X_test, y_test)\n",
    "\n",
    "# IMPLEMENT TWO MORE ENSEMBLE REGRESSORS!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Boosting!\n",
    "\n",
    "Experiment with an AdaBoostRegressor and interpret the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run time 0.2s\n",
    "\n",
    "# https://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_regression.html#sphx-glr-auto-examples-ensemble-plot-adaboost-regression-py\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "number_of_trees = 3 #put something suitable in here\n",
    "boosting = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=number_of_trees, random_state=0)\n",
    "boosting.fit(X_train, y_train)\n",
    "boosting.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run time 1m 13s to 3m\n",
    "\n",
    "fig, ax = plt.subplots(5,2)\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    axi.set_title(\"Tree {}\".format(i))\n",
    "    tree.plot_tree(boosting.estimators_[i], ax=axi, feature_names=dataset.feature_names, filled=True, rounded=True)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Random Forests\n",
    "\n",
    "Experiment with different parameters for the RF-Regressor. Test at least two different parameter sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "number_of_trees = 10\n",
    "forest = RandomForestRegressor(n_estimators=number_of_trees, random_state=0)\n",
    "forest.fit(X_train, y_train)\n",
    "forest.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for treeid in range(number_of_trees):\n",
    "    dot_data = tree.export_graphviz(forest.estimators_[treeid], feature_names=dataset.feature_names, out_file=None, filled=True, rounded=True, special_characters=True)\n",
    "    graph = graphviz.Source(dot_data, format=\"png\") \n",
    "    graph.render(\"forest_treeid\"+str(treeid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run time 1m 23s to 3m\n",
    "\n",
    "fig, ax = plt.subplots(5,2)\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    axi.set_title(\"Tree {}\".format(i))\n",
    "    tree.plot_tree(forest.estimators_[i], ax=axi, feature_names=dataset.feature_names, filled=True, rounded=True)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "63f08e0be87bb1ec22e3a665002567369c2bb78585d8d1135c35fb08381ea5a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
